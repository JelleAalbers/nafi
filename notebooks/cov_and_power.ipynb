{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage and power of simple experiments\n",
    "\n",
    "Jelle, October 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import nafi\n",
    "import nafi.high_level as nhl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# For precise CLS p-values\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Save all plots in this directory\n",
    "plot_dir = Path('plots_cov_and_power')\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_plot(name):\n",
    "    plt.savefig(plot_dir / f\"{name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, consider a Gaussian measurement $x \\sim \\mathrm{Normal}(\\mu, 1)$, with $\\mu \\geq 0$. \n",
    "\n",
    "This is the high-background limit of a counting experiment, i.e. constraining a signal $\\mu_s$ in the presence of a background $\\mu_b \\rightarrow \\infty$ (and $\\mu_b \\gg \\mu_s$) with the count $n \\sim \\mathrm{Poisson}(\\mu_s + \\mu_b) \\rightarrow \\mathrm{Normal}(\\mu_s + \\mu_b, \\sqrt{\\mu_b})$ or equivalently $x \\equiv \\frac{n - \\mu_b}{\\sqrt{\\mu_b}} \\sim \\mathrm{Normal}(\\mu_s, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider a finite range of mu_sig and x\n",
    "# Results will be unreliable for extreme x and high mu!\n",
    "x_range = (-10, 12)\n",
    "mu_sig = 12 * np.linspace(0, 1, 1_000)**2\n",
    "lnl, weights, x = nafi.likelihoods.gaussian.lnl_and_weights(\n",
    "    mu_sig,\n",
    "    n_x =10_000,\n",
    "    x_transform=lambda p: x_range[0] + (x_range[1] - x_range[0]) * p,\n",
    "    return_outcomes=True\n",
    ")\n",
    "\n",
    "gaussian = nhl.Experiment(\n",
    "    name=r\"High-background counting experiment\",\n",
    "    cl=0.9,\n",
    "    lnl=lnl,\n",
    "    weights=weights,\n",
    "    hypotheses=mu_sig,\n",
    "    outcomes=x,\n",
    "    #outcome_label=\"Observed x / $\\sigma$\",\n",
    "    outcome_label = \"Observed $(n - \\mu_{\\mathrm{bg}}) / \\sqrt{\\mu_\\mathrm{bg}}$\",\n",
    "    #hypothesis_label=\"Signal $\\mu \\;/\\; \\sigma$\",\n",
    "    hypothesis_label=\"$\\mu_\\mathrm{sig} \\;/\\; \\\\sqrt{\\mu_\\mathrm{bg}}$\",\n",
    "    outcome_plot_range=(-5, 5),\n",
    "    hypothesis_plot_range=(0, 4),\n",
    "    #singular_is_empty=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider a more sophisticated experiment that discriminates signal from background along some observable (say energy). Specifically, suppose the energy spectra of signal and background are equal-width Gaussians with a separation of three times their width.\n",
    "\n",
    "The second experiment has a background of 100 events. (The inifinite-background case would reduce back to the previous experiment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_bg = 100\n",
    "sigma_sep = 3\n",
    "\n",
    "# Signal rate hypotheses to consider: include 0 exactly, almost zero,\n",
    "# then 200 values at linearly increasing separation.\n",
    "res = 1.2\n",
    "mu_sig = np.cumsum(np.concatenate([\n",
    "    [0, .00001],\n",
    "    np.linspace(0.05/res, 0.3/res, int(100*res))]))\n",
    "n_max_sig = nafi.large_n_for_mu(mu_sig.max())\n",
    "\n",
    "lnl, weights = nafi.likelihoods.two_gaussians.lnl_and_weights(\n",
    "    mu_sig,\n",
    "    mu_bg=mu_bg,\n",
    "    n_sig_max=n_max_sig,\n",
    "    # TODO: 10k would be nicer...\n",
    "    trials_per_n=5_000,\n",
    "    sigma_sep=sigma_sep)\n",
    "\n",
    "gauss_sep = nhl.Experiment(\n",
    "    name=f'Two Gauss, {mu_bg}-event bg. at {sigma_sep}$\\sigma$ separation',\n",
    "    lnl=lnl,\n",
    "    weights=weights,\n",
    "    hypotheses=mu_sig,\n",
    "    hypothesis_label=\"Signal events $\\mu$\",\n",
    "    hypothesis_plot_range=(0, 6),\n",
    "    #singular_is_empty=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = dict(gaussian=gaussian, gauss_sep=gauss_sep)\n",
    "\n",
    "# Hack to avoid the region where CLs gives numerical errors.\n",
    "# Not needed for the plots below, and may require singular_is_empty=True above.\n",
    "# gaussian.get_results(nhl.CLs)\n",
    "# q = gaussian._results[nhl.CLs].p_outcome\n",
    "# q['mistake_ul'] = np.where(q['mistake_ul'] < 1e-5, np.nan, q['mistake_ul'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-sided methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below shows the size $\\alpha$ and rejection power $\\mathrm{RP}$ for simple, classic 90% confidence limit, upper limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xp_name, xp in experiments.items():\n",
    "    method_styles = {\n",
    "        nhl.ClassicUL: dict(color='g'),\n",
    "    }\n",
    "    nhl.power_vs_mistake(\n",
    "        xp, method_styles=method_styles, logit=False,\n",
    "        which='ul',\n",
    "        subplots=False, title=True,\n",
    "        linewidth=1, legend_kwargs=dict(frameon=True))\n",
    "    nhl.plots.brazil_hspan(yscale=100)\n",
    "    save_plot(f'{xp_name}_ul_alpha_rp_classiconly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly for some hypotheses $\\alpha \\approx \\mathrm{RP}$, i.e. exclusions are almost as likely when the hypothesis is true as when the dominant alternative ($\\mu = 0$) is true instead. Such exclusions would not be very informative.\n",
    "\n",
    "PCL and CLs both aim to avoid these cases. PCL maintains $\\alpha$ at nominal (10% for a 90% C.L.) unless $\\mathrm{RP}$ drops below some threshold (e.g. 50% for median-PCL), and if so, refuses to exclude any lower hypotheses. CLs instead requires $\\alpha/\\mathrm{RP} > \\alpha_\\mathrm{nominal}$, i.e. shrinks $\\alpha$ to $\\alpha = \\alpha_\\mathrm{nominal} \\cdot \\mathrm{RP}$.\n",
    "\n",
    "For the high-background case, either median-PCL or CLs may have a lower $\\alpha$, depending on the hypothesis. In the example with discrimination, CLs always has the lower $\\alpha$. Since they are still based on the same test statistic / ordering of observations, CLs limits in the latter case are strictly higher than the median-PCL limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xp_name, xp in experiments.items():\n",
    "\n",
    "    method_styles = {\n",
    "        nhl.ClassicUL: dict(color='g'),\n",
    "        nhl.ULPCLMedian: dict(color='b'),\n",
    "        nhl.CLs: dict(color='r'),\n",
    "        #nhl.BayesUL: dict(color='orange'),\n",
    "    }\n",
    "\n",
    "    nhl.power_vs_mistake(\n",
    "        xp, method_styles=method_styles, logit=False,\n",
    "        which='ul',\n",
    "        subplots=False, title=True,\n",
    "        linewidth=1, legend_kwargs=dict(frameon=True))\n",
    "    nhl.plots.brazil_hspan(yscale=100)\n",
    "    #plt.axhline(50, c='k', lw=1, alpha=0.2)\n",
    "    save_plot(f'{xp_name}_ul_alpha_rp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-sided methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want our methods to also give lower limits on appropriate results -- in case we are interested in actually discovering something ;-) -- there is a broader choice of methods.\n",
    "\n",
    "We have two basic methods:\n",
    "\n",
    "  * **Central intervals**: probability of excluding the truth is equally high for both limits (e.g. 5% for a 90% confidence interval);\n",
    "  * **Unified / Feldman-Cousins**: for each hypothesis, include outcomes in order of the likelihood ratio (the test statistic).\n",
    "\n",
    "These coincide only for trivial cases such as a free Gaussian measurement (with $\\mu \\in \\mathbb{R}$, not the Gaussian measurement with $\\mu \\geq 0$ we're considering). Otherwise, unified intervals have unequal error probabilities, i.e. the truth may be more likely to be above than below the interval. Likewise, central intervals do not order by the likelihood ratio, so there exist a pair of outcomes where only the _less_ extreme one (as judged by the test statistic) leads to a rejection of a hypothesis.\n",
    "\n",
    "Do not confuse central intervals with symmetric confidence intervals, i.e. where the upper and lower limit are equidistant from the best-fit. Those again arise only in special cases.\n",
    "\n",
    "The plot below shows the Neyman bands for these two methods, for the Gaussian experiment. For the discrimination experiment, we cannot sort and enumerate outcomes in an easy way to make this plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian.hypothesis_plot_range= (0, 6)\n",
    "gauss_sep.hypothesis_plot_range = (0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_styles = method_styles_base = {\n",
    "    nhl.FeldmanCousins: dict(color='steelblue'),\n",
    "    nhl.CentralIntervals: dict(color='seagreen'),\n",
    "}\n",
    "\n",
    "nhl.coverage_credibility_plot(gaussian, method_styles, lw=1.5)\n",
    "cov_ax = plt.gcf().get_axes()[-1]\n",
    "ax = cov_ax.twiny()\n",
    "ax.set_xlim(cov_ax.get_xlim())\n",
    "ax.set_xticks(cov_ax.get_xticks())\n",
    "ax.set_xticklabels([\"{:.0f}\".format((100 - p)) for p in cov_ax.get_xticks()])\n",
    "ax.set_xlabel(r\"$\\alpha$, type I error [%]\")\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "save_plot('twosided_bands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods can result in either an upper limit, or both an upper and a lower limit. Central intervals can also result in an empty result. Feldman-Cousins intervals never become empty since there is always some hypothesis for which the likelihood ratio is maximal. However, Feldman-Cousins upper limits can become very low (arbitrarily low?).\n",
    "\n",
    "PCL can be easily applied to two-sided methods: we simply constrain the upper limit based on the rejection power of the upper limit. For example, for a 90% central confidence interval, the upper limit is a 95% classic upper limit ($\\alpha = 5\\%$), for which [CCGV](https://arxiv.org/abs/1105.3166) recommended a $-1 \\sigma$ (16%) power constraint. \n",
    "\n",
    "Some experiments also prefer a **discovery threshold**: a requirement that an upper limit is only present once the p-value of the background-only hypothesis is below some threshold; we consider $3 \\sigma$ here. You can also consider combinations of PCL and a discovery threshold.\n",
    "\n",
    "The plots below show the coverage of our two different experiments, for no PCL, then $-1\\sigma$ PCL, and finally median-PCL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosided_outcome_plot(\n",
    "        xp, method_styles, outcome='mistake',\n",
    "        ylabel=r\"$\\alpha$, size, [%]\",\n",
    "        ylabel_complement=\"C.L., [%]\",\n",
    "        ylim=(0, 15),\n",
    "        yticks=(0, 5, 10, 15),\n",
    "        reference_line=10,\n",
    "        complement_axis=True,\n",
    "    ):\n",
    "\n",
    "    nrows = len(method_styles)\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(3, 0.5 + nrows), sharex=True)\n",
    "\n",
    "    for (ax_i, ax), (method, style) in zip(enumerate(axes), method_styles.items()):\n",
    "        r = xp.get_results(method)\n",
    "        plt.sca(ax)\n",
    "\n",
    "        if outcome == 'discovery':\n",
    "            p_ul = r.p_outcome['degenerate']\n",
    "            p_both = p_ul + r.p_outcome['discovery']\n",
    "        else:\n",
    "            p_both = r.p_outcome[outcome]\n",
    "            p_ul = r.p_outcome[outcome + '_ul']\n",
    "\n",
    "        plt.plot(xp.hypotheses, 100 * p_both, color=style['color'], lw=1)\n",
    "        plt.fill_between(xp.hypotheses, 0, 100 * p_ul, color=style['color'], lw=0, alpha=0.2)\n",
    "\n",
    "        if reference_line is not None:\n",
    "            plt.axhline(reference_line, color='k', lw=1, alpha=0.2)\n",
    "\n",
    "        show_ylabel = ax_i in (0, nrows - 1)\n",
    "\n",
    "        plt.xlim(xp.hypothesis_plot_range)\n",
    "        if show_ylabel:\n",
    "            plt.ylabel(ylabel)\n",
    "        ax.set_ylim(*ylim)\n",
    "        if yticks is None:\n",
    "            yticks = ax.get_yticks()\n",
    "        yticks = [q for q in yticks if ylim[0] <= q <= ylim[1]]\n",
    "        yticklabels = (\n",
    "            [yticks[0] if ax_i == nrows - 1 else \"\"]\n",
    "            + yticks[1:-1]\n",
    "            + [yticks[-1] if ax_i == 0 else f\"{yticks[0]}\\n{yticks[-1]}\"])\n",
    "        ax.set_yticks(yticks, yticklabels, fontsize=8)\n",
    "\n",
    "        # Complement axis\n",
    "        if complement_axis:\n",
    "            ax2 = plt.twinx()\n",
    "            ax2.set_ylim(ax.get_ylim())\n",
    "            if show_ylabel:\n",
    "                ax2.set_ylabel(ylabel_complement, alpha=0.5)\n",
    "            ax2.set_yticks(yticks)\n",
    "            # Label yticks with complements\n",
    "            yticks_c = (100 - np.asarray(yticks)).tolist()\n",
    "            ax2.set_yticklabels(\n",
    "                [yticks_c[0] if ax_i == nrows - 1 else f\"{yticks_c[0]}\\n{yticks_c[-1]}\"]\n",
    "                + yticks_c[1:-1]\n",
    "                + [yticks_c[-1] if ax_i == 0 else \"\"],\n",
    "                fontsize=8, alpha=0.5)\n",
    "\n",
    "        # Text on upper left corner\n",
    "        plt.text(0.05, 0.9, method.name, transform=ax.transAxes, fontsize=8, va='top', ha='left',\n",
    "                bbox=dict(facecolor='w', alpha=0.6, lw=0)\n",
    "                )\n",
    "        # Text in bottom right corner\n",
    "        plt.text(0.95, 0.07, \"Upper limit alone\", transform=ax.transAxes, fontsize=8, va='bottom', ha='right',\n",
    "                color=style['color'], alpha=0.5)\n",
    "\n",
    "    plt.sca(axes[-1])\n",
    "    plt.xlabel(xp.hypothesis_label)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "method_styles_vanilla = method_styles_base | {\n",
    "    nhl.FCThreeSigmaDisc: dict(color='goldenrod', linestyle='--', zorder=5),\n",
    "    nhl.CentralIntervalsThreeSigmaDisc: dict(color='orchid', linestyle=':', zorder=5, lw=2),\n",
    "    #nhl.CLsFlipFlopThreeSigma: dict(color='firebrick'),\n",
    "}\n",
    "\n",
    "method_styles_pclm1s = {\n",
    "    nhl.FeldmanCousinsPCLMinusOne: dict(color='steelblue'),\n",
    "    nhl.CentralIntervalsPCLMinusOne: dict(color='seagreen'),\n",
    "    nhl.FCThreeSigmaDiscPCLMinusOne: dict(color='goldenrod', linestyle='--', zorder=5),\n",
    "    nhl.CIThreeSigmaDiscPCLMinusOne: dict(color='orchid', linestyle=':', zorder=5, lw=2),\n",
    "}\n",
    "\n",
    "method_styles_pclmed = {\n",
    "    nhl.FeldmanCousinsPCLMedian: dict(color='steelblue'),\n",
    "    nhl.CentralIntervalsPCLMedian: dict(color='seagreen'),\n",
    "    nhl.FCThreeSigmaDiscPCLMedian: dict(color='goldenrod', linestyle='--', zorder=5),\n",
    "    nhl.CIThreeSigmaDiscPCLMedian: dict(color='orchid', linestyle=':', zorder=5, lw=2),\n",
    "}\n",
    "\n",
    "method_sets = [\n",
    "    ('nopcl', method_styles_vanilla),\n",
    "    ('pclm1s', method_styles_pclm1s),\n",
    "    ('pclmed', method_styles_pclmed),\n",
    "]\n",
    "\n",
    "for method_set_name, method_styles in method_sets:\n",
    "    print(method_set_name)\n",
    "    for xp_name, xp in experiments.items():\n",
    "        twosided_outcome_plot(xp, method_styles)\n",
    "        #plt.suptitle(xp.name, y=0.95, fontsize=10)\n",
    "        save_plot(f\"coverage_both_{xp_name}_{method_set_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Feldman-Cousins, PCL introduces funny curves in the coverage profile. For central intervals, PCL adds a simple step to the coverage -- it appears to be a steep diagonal in the discrimination experiment only because of the limited resolution of hypotheses we probed.\n",
    "\n",
    "A discovery threshold has the simpler effect of letting the size start near zero and then monotonously increasing towards the nominal value. (Coverage starts near 100% then decreases towards the nominal value.) Combining discovery threshold and PCL combines both effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLs is harder to generalize to two-sided methods. We consider two ways below:\n",
    "\n",
    "  * Naive flip-flop: publish a Feldman-Cousins or Central Intervals interval when a discovery threshold is met, otherwise a one-sided CLs upper limit. As with the flip-flop method originally considered in [Feldman & Cousins](https://arxiv.org/abs/physics/9711021), this has undercoverage for a large range of hypotheses and seems untenable.\n",
    "  * Clipped intervals: publish a Feldman-Cousins or Central Intervals interval, but raise the upper limit to the CLs upper limit if the latter is higher. This produces strictly conservative results (without undercoverage). A discovery threshold is optional (whereas it was mandatory for the flip-flop method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLsFlipFlopCIThreeSigma(nhl.FlipFlopMethod):\n",
    "    min_sigma = 3\n",
    "    method_if_discovery = nhl.CentralIntervals\n",
    "    method_otherwise = nhl.CLs\n",
    "\n",
    "method_styles = {\n",
    "    nhl.CLs: dict(color='r'),\n",
    "    nhl.CLsFlipFlopThreeSigma: dict(color='firebrick'),\n",
    "    CLsFlipFlopCIThreeSigma: dict(color='darksalmon'),\n",
    "    nhl.FeldmanCousinsCLsClipped: dict(color='peru'),\n",
    "    nhl.FCThreeSigmaCLsClipped: dict(color='saddlebrown'),\n",
    "    nhl.CICLsClipped: dict(color='maroon'),\n",
    "    nhl.CIThreeSigmaCLsClipped: dict(color='sienna'),\n",
    "}\n",
    "\n",
    "for xp_name, xp in experiments.items():\n",
    "    twosided_outcome_plot(xp, method_styles)\n",
    "    save_plot(f\"coverage_both_{xp_name}_cls_variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with PCl, the coverage profiles can be pretty funky. Central intervals clipped to CLs looks the simplest -- indeed, that looks like a smooth version of using central intervals with PCL at the median.\n",
    "\n",
    "Similarly, we can plot the rejection power for different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian.hypothesis_plot_range = (0, 4)\n",
    "gauss_sep.hypothesis_plot_range = (0, 6)\n",
    "\n",
    "for method_set_name, method_styles in method_sets:\n",
    "    print(method_set_name)\n",
    "    for xp_name, xp in experiments.items():\n",
    "        twosided_outcome_plot(xp, method_styles, 'excl_if_bg',\n",
    "                              ylabel='Rej.power [%]',\n",
    "                              ylim=(0, 60), yticks=(0, 20, 40, 60),\n",
    "                              reference_line=None, complement_axis=False)\n",
    "        save_plot(f\"rp_both_{xp_name}_{method_set_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rejection power of the upper limit is still monotonously increasing, as you would expect. The total rejection power of both limits is __not__ monotonous, unless a discovery treshold essentially eliminates the rejection power of the lower limit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
